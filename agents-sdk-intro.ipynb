{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1DGYFfoKvsB"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/gen-ai/openai/agents-sdk-intro.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG6pIZ-_7uRX"
      },
      "source": [
        "## OpenAI's Agents SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN2gNfry7tDp"
      },
      "source": [
        "OpenAI have released an **Agents SDK**, their version of an open source agent development library.\n",
        "\n",
        "OpenAI have outlined a few features of the library:\n",
        "\n",
        "```\n",
        "* Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\n",
        "* Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\n",
        "* Handoffs: A powerful feature to coordinate and delegate between multiple agents.\n",
        "* Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\n",
        "* Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\n",
        "* Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\n",
        "```\n",
        "\n",
        "([source](https://openai.github.io/openai-agents-python/))\n",
        "\n",
        "We'll focus on covering the essentials here - including the **agent loop**, **python-first**, **guardrails**, and **function tools** features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "outputs": [],
      "source": [
        "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI\n",
        "import os\n",
        "\n",
        "# Ensure the API key is set in the environment (if needed)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"lm-studio\"\n",
        "\n",
        "local_model = OpenAIChatCompletionsModel(\n",
        "    model=\"qwen2.5-14b-instruct@iq4_xs\",\n",
        "    openai_client=AsyncOpenAI(base_url=\"http://localhost:1234/v1\")\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=local_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-zOBFhAEROm"
      },
      "source": [
        "## Running our Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWO4ozC8ETNK"
      },
      "source": [
        "OpenAI gives us three methods for running our agent, all via a `Runner` class — those methods are:\n",
        "\n",
        "1. `Runner.run()` which runs in async.\n",
        "2. `Runner.run_sync()` which runs in sync.\n",
        "3. `Runner.run_streamed()` which runs in async _and_ streams the response back to us.\n",
        "\n",
        "We'll quicky test method **(1)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "4d2f3062-7d78-4dae-8b1e-ecfebb594ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Once upon a time, in a small village nestled between rolling hills and dense forests, there lived an old clockmaker named Eli. Known for his intricate clocks that could tell not just the hours but also the phases of the moon and the tides, Eli was revered by all.\\n\\nOne crisp autumn morning, as leaves painted the ground in hues of gold and crimson, a mysterious stranger arrived at Eli\\'s door. This visitor carried an ancient, ornate pocket watch with engravings that seemed to shimmer and change colors under different lights. The visitor explained he was on a quest to find someone who could repair this timepiece, which held great sentimental value.\\n\\nEli, intrigued by the challenge and the beauty of the watch, accepted the task. Days turned into weeks as Eli meticulously examined each part of the watch, ensuring it would function perfectly once again. As he worked, strange things began happening around him—flowers blooming at unusual times, shadows moving in peculiar patterns, and whispers carried on the wind that only Eli could hear.\\n\\nOne evening, while adjusting a tiny gear within the watch, Eli heard a soft voice say, \"Thank you.\" Startled but not frightened, he realized the watch was enchanted. It belonged to a time traveler who had lost it centuries ago during an adventure in this very village.\\n\\nWith the watch fully repaired and its magic restored, Eli handed it back to the stranger with a smile. The visitor thanked him profusely and promised that his name would be remembered by generations of travelers through time. As he left, the leaves rustled around Eli as if bidding farewell, and the whispers in the wind grew silent.\\n\\nFrom then on, Eli\\'s reputation spread far beyond the village walls, not just for his skill but also for his role in a tale of magic and time itself. And every year at that crisp autumn morning, he would look out over the hills, hoping to catch another glimpse of the mysterious traveler who had brought such wonder into his life.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_yue8fE4eX"
      },
      "source": [
        "In most scenarios we'll likely want to be using method **(3)**, ie running async and streaming tokens. To do this we need to write a little more code to handle the async streaming and print the tokens as they're returned.\n",
        "\n",
        "First, we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then _asynchronously_ iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "043f936a-7aea-4ec8-fdfd-a7aeb9ea2f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000002DA6732A990>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743203942.279833, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-14b-instruct@iq4_xs', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743203942.279833, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-14b-instruct@iq4_xs', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000002DA6732A990>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH-RoQfzMj6T"
      },
      "source": [
        "We can filter these various event types to find only raw tokens like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQANyvXG9zJ",
        "outputId": "8f64469d-fe14-4d74-be40-6a0d0af51848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Here's a short story for you:\n",
            "\n",
            "---\n",
            "\n",
            "**The Last Leaf**\n",
            "\n",
            "In the heart of the city, there was an old building with a single ivy-covered wall. On this wall, there hung one last leaf from a vine that had long since died.\n",
            "\n",
            "One day, a young artist named Lily moved into a small apartment on the ground floor of the building. She was battling a serious illness and felt her life slipping away. From her window, she could see the old ivy-covered wall with its solitary leaf.\n",
            "\n",
            "Days turned into weeks, and as autumn deepened, every night brought stronger winds that stripped the last remaining leaves from the vine. Each morning, Lily would wake up to find the single leaf still clinging to the wall, and it gave her a glimmer of hope.\n",
            "\n",
            "One stormy night, Lily's friend, an elderly painter named Mr. Johnson, noticed the wind was particularly fierce. He worried about Lily but also about that last leaf. After all, he had planted this vine many years ago when his wife was ill, hoping to give her something to hold onto.\n",
            "\n",
            "That night, as the storm raged on, Mr. Johnson secretly climbed up to the roof and attached a small artificial leaf to the wall where the real one hung. The next morning, Lily woke up to find that the last leaf was still there, undisturbed by the wind.\n",
            "\n",
            "Inspired by this symbol of resilience, Lily began to fight her illness with renewed vigor. She started painting again, capturing the beauty she saw in life and sharing it with others through her art.\n",
            "\n",
            "Months later, when spring arrived, Lily was well enough to venture outside. As she stood before the ivy-covered wall, she noticed something peculiar—the artificial leaf was still there, perfectly preserved amidst new growth. It was then that Mr. Johnson revealed his secret, explaining how he had kept the promise of hope alive for her.\n",
            "\n",
            "From that day on, Lily and Mr. Johnson became close friends, and their bond grew stronger with each passing year. The last leaf remained a symbol of enduring hope and the power of friendship to heal even the deepest wounds.\n",
            "\n",
            "---\n",
            "\n",
            "I hope you enjoyed this story!"
          ]
        }
      ],
      "source": [
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# we do need to reinitialize our runner before re-executing\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iixlv0sRMqr"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDHORYUcROZn"
      },
      "source": [
        "OpenAI included **function tools** as a key feature in their Agents SDK announcement. After turning everyone away from using _function calling_ to instead use _tool calling_, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "To use _function tools_ in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "outputs": [],
      "source": [
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "    answer.\"\"\"\n",
        "    return x*y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8b8KektTQbK"
      },
      "source": [
        "Note that we have taken extra care to include a clear and descriptive function name, relatively clear parameter names, type annotations for both input parameters and expected output, and a natural language docstring that will be fed to the LLM and explain to it _what_ this tool does.\n",
        "\n",
        "To run our agent _with_ tools we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=local_model,\n",
        "    tools=[multiply]  # note that we expect a list of tools\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtioNxflUS-s"
      },
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "579e3dc2-832c-4d35-da30-5bcbd757248b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001C063709E40>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743178938.287247, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-7b-instruct-1m@q8_0', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(id='__fake_id__', arguments='{\"x\": 7.814, \"y\": 103.892}', call_id='737969175', name='multiply', type='function_call', status=None), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"x\": 7.814, \"y\": 103.892}', item_id='__fake_id__', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(id='__fake_id__', arguments='{\"x\": 7.814, \"y\": 103.892}', call_id='737969175', name='multiply', type='function_call', status=None), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743178938.287247, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-7b-instruct-1m@q8_0', object='response', output=[ResponseFunctionToolCall(id='__fake_id__', arguments='{\"x\": 7.814, \"y\": 103.892}', call_id='737969175', name='multiply', type='function_call', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001C063709E40>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(id='__fake_id__', arguments='{\"x\": 7.814, \"y\": 103.892}', call_id='737969175', name='multiply', type='function_call', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001C063709E40>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': '737969175', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1743178939.073465, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-7b-instruct-1m@q8_0', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' result', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' of', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' multiplying', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='7', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='1', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='4', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' by', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='1', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='0', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='3', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='9', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='2', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' is', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='1', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='1', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='1', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='2', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='0', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='8', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is 811.812088.', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is 811.812088.', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1743178939.073465, error=None, incomplete_details=None, instructions=None, metadata=None, model='qwen2.5-7b-instruct-1m@q8_0', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is 811.812088.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001C063709E40>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is 811.812088.', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ],
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQZu6iyPU0wG"
      },
      "source": [
        "If we look closely at the fourth event object we will see `ResponseFunctionToolCall`, meaning our `multiply` tool was called by our LLM. Following this event object we can also see several events containing the `ResponseFunctionCallArgumentsDeltaEvent` type inside the `data` field — these are the input parameters for our tool.\n",
        "\n",
        "Let's rerun that but this time we will process the event outputs to generate a cleaner and more readable output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhFvNVowUrd5",
        "outputId": "c572523c-e86a-4925-9010-367387766b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Current Agent: Assistant\n",
            "{\"x\": 7.814, \"y\": 103.892}\n",
            "> Tool Called, name: multiply\n",
            "> Tool Called, args: {\"x\": 7.814, \"y\": 103.892}\n",
            "> Tool Output: 811.812088\n",
            "The result of multiplying 7.814 by 103.892 is approximately 811.8121."
          ]
        }
      ],
      "source": [
        "from openai.types.responses import (\n",
        "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
        "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
        ")\n",
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
        "            # this is streamed parameters for our tool call\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            # this is streamed final answer tokens\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        # this tells us which agent is currently in use\n",
        "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # these are events containing info that we'd typically\n",
        "        # stream out to a user or some downstream process\n",
        "        if event.name == \"tool_called\":\n",
        "            # this is the collection of our _full_ tool call after our tool\n",
        "            # tokens have all been streamed\n",
        "            print()\n",
        "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
        "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
        "        elif event.name == \"tool_output\":\n",
        "            # this is the response from our tool execution\n",
        "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqqWET4O93il"
      },
      "source": [
        "## Guardrails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrf2Ao396Dg"
      },
      "source": [
        "OpenAI have also included guardrails in the Agents SDK. These come as _input guardrails_ and _output guardrails_, the `input_guardrail` checks that the input going into your LLM is \"safe\" and the `output_guardrail` checks that the output from your LLM is \"safe\".\n",
        "\n",
        "Let's see how to use them. First, we'll implement a guardrail powered by another LLM (more tokens means more $$$ for OpenAI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I1LT_UpiXFg0"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# define structure of output for any guardrail agents\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# define an agent that checks if user is asking about political opinions\n",
        "politics_agent = Agent(\n",
        "    name=\"Politics check\",\n",
        "    instructions=\"Check if the user is asking you about political opinions\",\n",
        "    output_type=GuardrailOutput,\n",
        "    model= local_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRRFcCFGBHSk"
      },
      "source": [
        "We can call this agent directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4kgYMC9A7WP",
        "outputId": "7660cdce-00f3-41c9-812c-691fd8a85919"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunResult(input='what do you think about the labour party in the UK?', new_items=[MessageOutputItem(agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='{\\n  \"is_triggered\": true,\\n  \"reasoning\": \"The user is seeking an opinion on a specific political entity, which indicates they are interested in political views.\"\\n}', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='{\\n  \"is_triggered\": true,\\n  \"reasoning\": \"The user is seeking an opinion on a specific political entity, which indicates they are interested in political views.\"\\n}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=35, output_tokens=39, total_tokens=74), referenceable_id=None)], final_output=GuardrailOutput(is_triggered=True, reasoning='The user is seeking an opinion on a specific political entity, which indicates they are interested in political views.'), input_guardrail_results=[], output_guardrail_results=[], _last_agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001C063703BD0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what do you think about the labour party in the UK?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NfFXKYOBeJO"
      },
      "source": [
        "The output from our agent is hidden away in there, we extract it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrS4dT5mC7jN",
        "outputId": "6e464d4f-ecbb-4aed-c0d4-208fcfcc9f17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GuardrailOutput(is_triggered=True, reasoning='The user is seeking an opinion on a specific political entity, which indicates they are interested in political views.')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okpF-4pAC_zC"
      },
      "source": [
        "To integrate this with our other agents we need to move our logic into a single function decorated with the `@input_guardrail` decorator.\n",
        "\n",
        "When defining these guardrails we need to follow the following structure:\n",
        "\n",
        "* Input parameters must include a `ctx` (context), `agent`, and `input` (the user's query in this case). Note that below we will only use the `input` parameter.\n",
        "* Output must be a `GuardrailFunctionOutput` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o3UQIt-qBLTK"
      },
      "outputs": [],
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")\n",
        "\n",
        "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
        "@input_guardrail\n",
        "async def politics_guardrail(\n",
        "    ctx: RunContextWrapper[None],\n",
        "    agent: Agent,\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # run agent to check if guardrail is triggered\n",
        "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
        "    # format response into GuardrailFunctionOutput\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8eCZluTFnYS"
      },
      "source": [
        "Now we can initialize our normal agent with the `input_guardrails` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_JYELjOSFoED"
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=local_model,\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxYq9Aq0GIv6"
      },
      "source": [
        "Now let's run it! We'll stick with `Runner.run` for the sake of brevity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GKDUcBYMGGcG",
        "outputId": "e512f6d0-fabb-44a0-c5ce-7a591d2824e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5sN23B_HcPn"
      },
      "source": [
        "Let's see if our guardrail will trigger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "-tncyfxYGdRn",
        "outputId": "c35a415e-0d41-487d-9753-cf0151350039"
      },
      "outputs": [
        {
          "ename": "InputGuardrailTripwireTriggered",
          "evalue": "Guardrail InputGuardrail triggered tripwire",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInputGuardrailTripwireTriggered\u001b[0m           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m      2\u001b[0m     starting_agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat do you think about the labour party in the UK?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\kritt\\.conda\\envs\\ai_agents\\Lib\\site-packages\\agents\\run.py:210\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[0;32m    205\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 210\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[0;32m    212\u001b[0m             starting_agent,\n\u001b[0;32m    213\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[0;32m    215\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[0;32m    216\u001b[0m             context_wrapper,\n\u001b[0;32m    217\u001b[0m         ),\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    219\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    220\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    221\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[0;32m    222\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    223\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    224\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    225\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[0;32m    226\u001b[0m         ),\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    230\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    231\u001b[0m         original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m         should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[0;32m    237\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\kritt\\.conda\\envs\\ai_agents\\Lib\\site-packages\\agents\\run.py:805\u001b[0m, in \u001b[0;36mRunner._run_input_guardrails\u001b[1;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[0;32m    798\u001b[0m         t\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m    799\u001b[0m     _utils\u001b[38;5;241m.\u001b[39mattach_error_to_current_span(\n\u001b[0;32m    800\u001b[0m         SpanError(\n\u001b[0;32m    801\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail tripwire triggered\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    802\u001b[0m             data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguardrail\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mguardrail\u001b[38;5;241m.\u001b[39mget_name()},\n\u001b[0;32m    803\u001b[0m         )\n\u001b[0;32m    804\u001b[0m     )\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InputGuardrailTripwireTriggered(result)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     guardrail_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
            "\u001b[1;31mInputGuardrailTripwireTriggered\u001b[0m: Guardrail InputGuardrail triggered tripwire"
          ]
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what do you think about the labour party in the UK?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvGzaTDJHmts"
      },
      "source": [
        "Great, our guardrail triggered! The `output_guardrail` type is implemented in almost the exact same way, but uses the `@output_guardrail` decorator when defining the guardrail function, and the `output_guardrails` parameter when defining our `Agent`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnVc_II-IcRg"
      },
      "source": [
        "## Conversational Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EV4EtGZIevd"
      },
      "source": [
        "So far we've only seen how to use our agents with single messages. Many use-cases require chat history to make our agents conversational. To implement that we simply provide a list of messages to our `Runner`.\n",
        "\n",
        "Let's see how this works, first we send a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wK558-qII5WA",
        "outputId": "5eaf5b88-18ef-4d4b-82ac-bf8511d519f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Sure, I've noted the number 7.814 for you. How can I assist you further?\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"remember the number 7.814 for me please\"\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_kDKxFI5LO"
      },
      "source": [
        "Fortunately, we can help our agent remember this information. We can use the `.to_input_list()` method to format our `result` into a list of messages for our next query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-buAAvJh8F",
        "outputId": "3b889236-2a26-4c91-bcca-45733518241d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'remember the number 7.814 for me please', 'role': 'user'},\n",
              " {'id': '__fake_id__',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': \"Sure, I've noted the number 7.814 for you. How can I assist you further?\",\n",
              "    'type': 'output_text'}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_input_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia1n7UG_JkYh"
      },
      "source": [
        "We merge this with our next message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y4WKSTObJokR",
        "outputId": "01331fad-5e8c-4e64-86cd-a3470432d462"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812. Is there anything else you need help with?'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=result.to_input_list() + [\n",
        "        {\"role\": \"user\", \"content\": \"multiply the last number by 103.892\"}\n",
        "    ]\n",
        ")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bGdZMCLKDMK"
      },
      "source": [
        "It looks like our agent can remember our previous interactions after all!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khGNoTd_KH4k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFi__V9hKIKQ"
      },
      "source": [
        "That is our rapid-fire overview of OpenAI's new Agents SDK. We've covered most of the essentials here but there are many other features in the library, and many of the features we included here come with plenty of different ways to use. The SDK is already fairly substantial and certainly worth keeping an eye on."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
