{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents in OpenAI: A Structured Overview\n",
    "\n",
    "This document provides an overview of Agents in the OpenAI ecosystem, drawing from various resources.\n",
    "\n",
    "### 1. Core Concepts\n",
    "\n",
    "*   **Definition:** Agents are a fundamental building block, essentially LLMs configured with specific instructions and tools. (Reference: 2)\n",
    "\n",
    "*   **Key Properties:**\n",
    "    *   `instructions`: (also known as a developer message or system prompt) Directives that guide the Agent's behavior. (Reference: 2)\n",
    "    *   `model`: The specific LLM to be used (e.g., \"o3-mini\"). (Reference: 2)\n",
    "    *   `model_settings`: Configuration options to tune the LLM's behavior, such as temperature and top\\_p. (Reference: 2)\n",
    "    *   `tools`: Functions or capabilities the Agent can use to achieve its goals. (Reference: 2)\n",
    "    *   `context`: A dependency-injection mechanism allowing users to provide dependencies and state to each agent during the run. (Reference: 2)\n",
    "    *   `output_type`: The desired format of the agent's output. If not set, the default output type is plain text (i.e., `str`). Another common choice is Pydantic objects. (Reference: 2)\n",
    "\n",
    "### 2. Basic Agent Configuration (Reference: 2)\n",
    "\n",
    "The following code snippet demonstrates the basic configuration of an agent:\n",
    "\n",
    "```python\n",
    "from agents import Agent, ModelSettings, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is sunny\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Haiku agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    model=\"o3-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Advanced Features\n",
    "\n",
    "*   **Handoffs:** Agents can delegate tasks to specialized sub-agents called \"handoffs\".  This allows for modular and specialized agent orchestration. (Reference: 2)\n",
    "\n",
    "*   **Dynamic Instructions:** Instructions can be provided dynamically via a function that receives the agent and context and returns the prompt. (Reference: 2)\n",
    "\n",
    "*   **Lifecycle Events (Hooks):**  Allows observation of the Agent's lifecycle.  You can hook into the agent lifecycle with the `hooks` property by subclassing the `AgentHooks` class and overriding the methods you're interested in. (Reference: 2)\n",
    "\n",
    "*   **Guardrails:** Mechanisms for running checks/validations on user input, in parallel with the agent's execution. (Reference: 2)\n",
    "\n",
    "*   **Cloning/Copying Agents:** Agents can be duplicated using the `clone()` method, with the option to modify properties during the cloning process. (Reference: 2)\n",
    "\n",
    "### 4. Forcing Tool Use (Reference: 2)\n",
    "\n",
    "The `ModelSettings.tool_choice` parameter can force the LLM to use tools. Valid values include:\n",
    "\n",
    "*   `auto`: LLM decides whether to use a tool.\n",
    "*   `required`: LLM must use a tool (it chooses which one).\n",
    "*   `none`: LLM must not use a tool.\n",
    "*   `my_tool`: (a specific string) LLM must use the specified tool.\n",
    "\n",
    "**Important Note:** When forcing tool use, consider setting `Agent.tool_use_behavior` to prevent infinite loops where the LLM is perpetually forced to call a tool.\n",
    "\n",
    "### 5. Additional Resources (References)\n",
    "\n",
    "*   [agents-sdk-intro.ipynb](https://github.com/aurelio-labs/cookbook/blob/main/gen-ai/openai/agents-sdk-intro.ipynb): A cookbook example demonstrating the Agent SDK. (Reference: 1)\n",
    "*   [agents.md](https://github.com/openai/openai-agents-python/blob/main/docs/agents.md): Documentation on Agents in the OpenAI Agents Python library. (Reference: 2)\n",
    "*   [Quickstart](https://openai.github.io/openai-agents-python/quickstart/): Quickstart guide for the OpenAI Agents Python library. (Reference: 3)\n",
    "*   [Model Context Protocol](https://docs.continue.dev/customize/context-providers#model-context-protocol): Information on the Model Context Protocol. (Reference: 4)\n",
    "*   [End-to-End Examples](https://openai.github.io/openai-agents-python/mcp/#end-to-end-examples): End-to-end examples of using the Model Context Protocol. (Reference: 6)\n",
    "*   [YouTube Video](https://www.youtube.com/watch?v=35nxORG1mtg): A video resource (unspecified content). (Reference: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunResultStreaming(input='What is higher between 9.19 and 9.9?', new_items=[], raw_responses=[], final_output=None, input_guardrail_results=[], output_guardrail_results=[], current_agent=Agent(name='Assistant', instructions='You are a helpful reasoning assistant', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x10e6908d0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None, max_tokens=None), tools=[], mcp_servers=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), current_turn=0, max_turns=10, is_complete=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows asyncio.run() to work in Jupyter\n",
    "\n",
    "local_model = OpenAIChatCompletionsModel(\n",
    "    model=\"qwen2.5-14b-instruct-1m\",\n",
    "    openai_client=AsyncOpenAI(base_url=\"http://localhost:1234/v1\", \n",
    "                              api_key='lm-studio')\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful reasoning assistant\",\n",
    "    model=local_model\n",
    ")\n",
    "\n",
    "async def stream_response():\n",
    "    result = Runner.run_streamed(agent, \"What is higher between 9.19 and 9.9?\")\n",
    "    # Use stream_events() instead of stream\n",
    "    async for event in result.stream_events():\n",
    "        # Handle raw response events for text streaming\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "# Now you can use asyncio.run() as normal\n",
    "asyncio.run(stream_response())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the heart of a small, forgotten village, there was an old clock that stood tall in the town square. Its hands moved slowly, as if it had seen too much time pass by. The villagers believed the clock held a secret — that at midnight, on the stroke of 12, it would tell its most ancient tale.\\n\\nOne stormy night, a young girl named Lila, curious and brave, decided to stay up late to witness the moment. As the winds howled and rain lashed against her window, she watched the clock\\'s hands inch closer to midnight. When the final chime tolled, the clock\\'s face glowed faintly, and a soft voice filled the air.\\n\\n\"It was built by a man who lost his love,\" the voice said. \"He crafted it to remember every moment, hoping time would heal his heart.\"\\n\\nLila smiled, understanding that sometimes, the greatest stories aren’t told in words but in the quiet moments we cherish. She wrapped herself in her blanket, feeling a warmth that came from knowing there was magic in the world — even if it only lived in old clocks and sleepy nights.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\"\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village by the sea, there lived an old fisherman named Tom. Every morning, he would set out in his creaky boat, hoping to catch just enough fish to feed his family. One foggy morning, as he cast his net into the misty waters, something unusual happened. The fish didn’t bite that day, but when he pulled his net back into the boat, it was filled with shimmering, golden coins instead.\n",
      "\n",
      "Tom was puzzled but grateful. He knew he couldn’t keep the treasure, so he returned to his village and shared it with the townspeople. Together, they built a school, fixed the church roof, and helped those in need. From then on, whenever Tom cast his net, he always made sure to leave a little extra for those who needed it most. And though the coins never returned, something even more precious did—the warmth of a community that stood together.\n",
      "\n",
      "And so, Tom learned that the greatest catch wasn’t in the ocean, but in the hearts of those around him."
     ]
    }
   ],
   "source": [
    "# we do need to reinitialize our runner before re-executing\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"tell me a short story\"\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and \\\n",
    "        isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
    "    answer.\"\"\"\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = agent.clone(\n",
    "    name=\"Math Agent\",\n",
    "        instructions=(\n",
    "        \"You're a helpful assistant, remember to always \"\n",
    "        \"use the provided tools whenever possible. Do not \"\n",
    "        \"rely on your own knowledge too much and instead \"\n",
    "        \"use your tools to help you answer queries.\"\n",
    "    ),\n",
    "    tools=[multiply]  # note that we expect a list of tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the product of 7.814 and 103.909, you simply multiply the two numbers together:\n",
      "\n",
      "\\[ 7.814 \\times 103.909 = 812.506726 \\]\n",
      "\n",
      "So, 7.814 multiplied by 103.909 is approximately **812.507** when rounded to three decimal places."
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\"what is 7.814 multiplied by 103.909?\"\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and \\\n",
    "        isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 7.814 by 103.909 is approximately 811.945."
     ]
    }
   ],
   "source": [
    "response = Runner.run_streamed(\n",
    "    starting_agent=math_agent,\n",
    "    input=\"what is 7.814 multiplied by 103.909?\"\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\" and \\\n",
    "        isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Current Agent: Math Agent\n",
      "{\"x\": 7.814, \"y\": 103.892}\n",
      "> Tool Called, name: multiply\n",
      "> Tool Called, args: {\"x\": 7.814, \"y\": 103.892}\n",
      "> Tool Output: 811.812088\n",
      "The result of multiplying 7.814 by 103.892 is approximately 811.812."
     ]
    }
   ],
   "source": [
    "from openai.types.responses import (\n",
    "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
    "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
    ")\n",
    "\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=math_agent,\n",
    "    input=\"what is 7.814 multiplied by 103.892?\"\n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\":\n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            # this is streamed parameters for our tool call\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            # this is streamed final answer tokens\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # this tells us which agent is currently in use\n",
    "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # these are events containing info that we'd typically\n",
    "        # stream out to a user or some downstream process\n",
    "        if event.name == \"tool_called\":\n",
    "            # this is the collection of our _full_ tool call after our tool\n",
    "            # tokens have all been streamed\n",
    "            print()\n",
    "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
    "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # this is the response from our tool execution\n",
    "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
